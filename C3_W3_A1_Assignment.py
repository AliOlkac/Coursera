import time
from collections import deque, namedtuple

import numpy as np
import PIL.Image
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.losses import MSE
from tensorflow.keras.optimizers import Adam


# UNQ_C1
# GRADED CELL

# Create the Q-Network
q_network = Sequential([
    ### START CODE HERE ###
    Input(shape=(state_size,)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(action_size, activation='linear')

    ### END CODE HERE ###
])

# Create the target Q^-Network
target_q_network = Sequential([
    ### START CODE HERE ###
    Input(shape=(state_size,)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(action_size, activation='linear')


    ### END CODE HERE ###
])

### START CODE HERE ###
optimizer = None
### END CODE HERE ###


# UNQ_C2
# GRADED FUNCTION: calculate_loss

def compute_loss(experiences, gamma, q_network, target_q_network):
    """
    Calculates the loss.

    Args:
      experiences: (tuple) tuple of ["state", "action", "reward", "next_state", "done"] namedtuples
      gamma: (float) The discount factor.
      q_network: (tf.keras.Sequential) Keras model for predicting the q_values
      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets

    Returns:
      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between
            the y targets and the Q(s,a) values.
    """

    # Unpack the mini-batch of experience tuples
    states, actions, rewards, next_states, done_vals = experiences

    # Compute max Q^(s,a)
    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)

    # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).
    ### START CODE HERE ###
    y_targets = None
    if done_vals:
        y_targets = rewards
    else:
        y_targets = rewards + (gamma * max_qsa)

    ### END CODE HERE ###

    # Get the q_values and reshape to match y_targets
    q_values = q_network(states)
    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),
                                                tf.cast(actions, tf.int32)], axis=1))

    # Compute the loss
    ### START CODE HERE ###
    loss = None
    loss = MSE(y_targets, q_values)
    ### END CODE HERE ###

    return loss